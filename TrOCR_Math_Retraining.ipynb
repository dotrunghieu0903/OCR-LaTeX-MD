{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00baec73",
   "metadata": {},
   "source": [
    "# Retraining TrOCR_Math_handwritten on Google Mathwriting Dataset (2024)\n",
    "\n",
    "This notebook provides a comprehensive guide to re-train and evaluate the [TrOCR_Math_handwritten](https://huggingface.co/fhswf/TrOCR_Math_handwritten) model using the Google Mathwriting dataset from https://storage.googleapis.com/mathwriting_data/mathwriting-2024.tgz."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4257275",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f2c543",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install necessary packages\n",
    "!pip install transformers datasets tokenizers accelerate evaluate jiwer matplotlib tqdm pillow gitpython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b9fdb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "from git import Repo\n",
    "\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from transformers import (\n",
    "    TrOCRProcessor, \n",
    "    VisionEncoderDecoderModel, \n",
    "    Seq2SeqTrainer, \n",
    "    Seq2SeqTrainingArguments,\n",
    "    default_data_collator\n",
    ")\n",
    "from transformers.modeling_outputs import Seq2SeqLMOutput\n",
    "import evaluate\n",
    "from datasets import load_dataset, Dataset as HFDataset\n",
    "\n",
    "# Check if CUDA is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14381709",
   "metadata": {},
   "source": [
    "## 2. Download and Prepare the Dataset\n",
    "\n",
    "We'll download the mathwriting dataset from Google Cloud Storage and prepare it for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92670829",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the direct URL for the mathwriting dataset\n",
    "mathwriting_url = \"https://storage.googleapis.com/mathwriting_data/mathwriting-2024.tgz\"\n",
    "dataset_filename = \"mathwriting-2024.tgz\"\n",
    "data_dir = \"mathwriting_data\"\n",
    "\n",
    "# Download the dataset if not already present\n",
    "if not os.path.exists(dataset_filename):\n",
    "    print(f\"Downloading dataset from {mathwriting_url}...\")\n",
    "    !curl -L {mathwriting_url} -o {dataset_filename}\n",
    "    print(\"Download complete!\")\n",
    "else:\n",
    "    print(f\"Dataset file {dataset_filename} already exists.\")\n",
    "\n",
    "# Create data directory if it doesn't exist\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "# Extract the dataset if not already extracted\n",
    "if not os.path.exists(os.path.join(data_dir, \"extracted\")):\n",
    "    print(\"Extracting dataset...\")\n",
    "    !tar -xzf {dataset_filename} -C {data_dir}\n",
    "    # Create a marker file to indicate extraction is complete\n",
    "    Path(os.path.join(data_dir, \"extracted\")).touch()\n",
    "    print(\"Extraction complete!\")\n",
    "else:\n",
    "    print(\"Dataset already extracted.\")\n",
    "\n",
    "# Check the structure of the extracted data\n",
    "print(\"\\nExploring dataset structure:\")\n",
    "!ls -la {data_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9a98e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to explore dataset structure\n",
    "def explore_dataset_structure(base_dir):\n",
    "    \"\"\"Print information about the dataset structure.\"\"\"\n",
    "    print(f\"Exploring dataset structure in {base_dir}\")\n",
    "    \n",
    "    # Count files by extension\n",
    "    extension_counts = {}\n",
    "    total_files = 0\n",
    "    \n",
    "    # Find directories and subdirectories\n",
    "    directories = set()\n",
    "    \n",
    "    for root, dirs, files in os.walk(base_dir):\n",
    "        rel_root = os.path.relpath(root, base_dir)\n",
    "        if rel_root != \".\":\n",
    "            directories.add(rel_root)\n",
    "        \n",
    "        for file in files:\n",
    "            total_files += 1\n",
    "            ext = os.path.splitext(file)[1].lower()\n",
    "            extension_counts[ext] = extension_counts.get(ext, 0) + 1\n",
    "    \n",
    "    # Print summary\n",
    "    print(f\"\\nFound {total_files} total files in {len(directories)} directories\")\n",
    "    print(\"\\nDirectory structure:\")\n",
    "    for i, directory in enumerate(sorted(directories)[:10]):  # Show first 10 directories\n",
    "        print(f\"- {directory}\")\n",
    "    if len(directories) > 10:\n",
    "        print(f\"... and {len(directories) - 10} more directories\")\n",
    "    \n",
    "    print(\"\\nFile extensions:\")\n",
    "    for ext, count in sorted(extension_counts.items(), key=lambda x: x[1], reverse=True):\n",
    "        print(f\"- {ext}: {count} files\")\n",
    "    \n",
    "    # Check if we have image and annotation pairs\n",
    "    if '.png' in extension_counts and '.txt' in extension_counts:\n",
    "        # Find a sample image and check for its annotation\n",
    "        for root, _, files in os.walk(base_dir):\n",
    "            for file in files:\n",
    "                if file.endswith('.png'):\n",
    "                    img_path = os.path.join(root, file)\n",
    "                    base_name = os.path.splitext(file)[0]\n",
    "                    txt_path = os.path.join(root, base_name + '.txt')\n",
    "                    \n",
    "                    if os.path.exists(txt_path):\n",
    "                        print(\"\\nFound a matching image-annotation pair:\")\n",
    "                        print(f\"Image: {img_path}\")\n",
    "                        print(f\"Annotation: {txt_path}\")\n",
    "                        \n",
    "                        # Display annotation content\n",
    "                        try:\n",
    "                            with open(txt_path, 'r', encoding='utf-8') as f:\n",
    "                                content = f.read().strip()\n",
    "                            print(f\"Annotation content: {content[:100]}{'...' if len(content) > 100 else ''}\")\n",
    "                        except Exception as e:\n",
    "                            print(f\"Error reading annotation: {e}\")\n",
    "                        \n",
    "                        return  # Exit after finding one example\n",
    "\n",
    "# Run the exploration after extraction\n",
    "if os.path.exists(os.path.join(data_dir, \"extracted\")):\n",
    "    explore_dataset_structure(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eebc0158",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the mathwriting-2024 dataset structure\n",
    "def process_mathwriting_dataset(data_dir):\n",
    "    \"\"\"\n",
    "    Process the mathwriting-2024 dataset and organize it for training.\n",
    "    \n",
    "    Expected structure of the mathwriting dataset:\n",
    "    - Images with handwritten math expressions\n",
    "    - Corresponding text files with LaTeX annotations\n",
    "    \"\"\"\n",
    "    print(\"Processing mathwriting-2024 dataset...\")\n",
    "    \n",
    "    # Create directories for organized data\n",
    "    images_dir = os.path.join(data_dir, \"processed\", \"images\")\n",
    "    os.makedirs(images_dir, exist_ok=True)\n",
    "    \n",
    "    # Initialize lists to store data\n",
    "    all_image_paths = []\n",
    "    all_formulas = []\n",
    "    \n",
    "    # Walk through the extracted data directory\n",
    "    for root, dirs, files in os.walk(data_dir):\n",
    "        for file in files:\n",
    "            # Skip the marker file and other non-relevant files\n",
    "            if file == \"extracted\" or not (file.endswith('.png') or file.endswith('.jpg') or file.endswith('.jpeg')):\n",
    "                continue\n",
    "            \n",
    "            # Get image file path\n",
    "            image_file = os.path.join(root, file)\n",
    "            \n",
    "            # Find corresponding annotation file (usually a text file with the same name)\n",
    "            base_name = os.path.splitext(file)[0]\n",
    "            annotation_file = None\n",
    "            \n",
    "            # Check for different possible annotation extensions\n",
    "            for ext in ['.txt', '.tex', '.latex']:\n",
    "                potential_file = os.path.join(root, base_name + ext)\n",
    "                if os.path.exists(potential_file):\n",
    "                    annotation_file = potential_file\n",
    "                    break\n",
    "            \n",
    "            # If we found both image and annotation, add to our dataset\n",
    "            if annotation_file:\n",
    "                try:\n",
    "                    # Read the formula from the annotation file\n",
    "                    with open(annotation_file, 'r', encoding='utf-8') as f:\n",
    "                        formula = f.read().strip()\n",
    "                    \n",
    "                    # Copy the image to our organized directory\n",
    "                    dest_image_path = os.path.join(images_dir, file)\n",
    "                    if not os.path.exists(dest_image_path):\n",
    "                        shutil.copy(image_file, dest_image_path)\n",
    "                    \n",
    "                    # Add to our lists\n",
    "                    all_image_paths.append(dest_image_path)\n",
    "                    all_formulas.append(formula)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {image_file}: {e}\")\n",
    "    \n",
    "    print(f\"Found {len(all_image_paths)} valid image-formula pairs\")\n",
    "    \n",
    "    # Create train/val/test splits\n",
    "    indices = list(range(len(all_image_paths)))\n",
    "    np.random.shuffle(indices)\n",
    "    \n",
    "    # Split data: 80% train, 10% val, 10% test\n",
    "    train_size = int(0.8 * len(indices))\n",
    "    val_size = int(0.1 * len(indices))\n",
    "    \n",
    "    train_indices = indices[:train_size]\n",
    "    val_indices = indices[train_size:train_size + val_size]\n",
    "    test_indices = indices[train_size + val_size:]\n",
    "    \n",
    "    # Create split datasets\n",
    "    splits = {\n",
    "        'train': [all_image_paths[i] for i in train_indices],\n",
    "        'train_formulas': [all_formulas[i] for i in train_indices],\n",
    "        'val': [all_image_paths[i] for i in val_indices],\n",
    "        'val_formulas': [all_formulas[i] for i in val_indices],\n",
    "        'test': [all_image_paths[i] for i in test_indices],\n",
    "        'test_formulas': [all_formulas[i] for i in test_indices]\n",
    "    }\n",
    "    \n",
    "    # Save the splits info\n",
    "    import json\n",
    "    for split_name in ['train', 'val', 'test']:\n",
    "        split_data = []\n",
    "        for img_path, formula in zip(splits[split_name], splits[f\"{split_name}_formulas\"]):\n",
    "            split_data.append({\n",
    "                'image_path': img_path,\n",
    "                'formula': formula\n",
    "            })\n",
    "        \n",
    "        with open(os.path.join(data_dir, f\"{split_name}.json\"), 'w') as f:\n",
    "            json.dump(split_data, f)\n",
    "    \n",
    "    print(f\"Data splits created: Train {len(splits['train'])} / Val {len(splits['val'])} / Test {len(splits['test'])}\")\n",
    "    return splits\n",
    "\n",
    "# Process the dataset\n",
    "try:\n",
    "    splits = process_mathwriting_dataset(data_dir)\n",
    "    train_images, train_formulas = splits['train'], splits['train_formulas']\n",
    "    val_images, val_formulas = splits['val'], splits['val_formulas']\n",
    "    test_images, test_formulas = splits['test'], splits['test_formulas']\n",
    "except Exception as e:\n",
    "    print(f\"Error processing dataset: {e}\")\n",
    "    print(\"Falling back to an alternative approach...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "009a6511",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fallback method in case the direct dataset processing fails\n",
    "def process_mathwriting_fallback(data_dir):\n",
    "    \"\"\"\n",
    "    Alternative method to process the mathwriting data if the standard approach fails.\n",
    "    This function attempts different strategies to locate and organize the data.\n",
    "    \"\"\"\n",
    "    import shutil\n",
    "    import json\n",
    "    \n",
    "    print(\"Using fallback method to process dataset...\")\n",
    "    \n",
    "    # Create output directories\n",
    "    os.makedirs(os.path.join(data_dir, \"processed\", \"images\"), exist_ok=True)\n",
    "    \n",
    "    # Try to find image files recursively\n",
    "    image_files = []\n",
    "    for ext in ['.png', '.jpg', '.jpeg']:\n",
    "        for root, _, files in os.walk(data_dir):\n",
    "            for file in files:\n",
    "                if file.endswith(ext):\n",
    "                    image_files.append(os.path.join(root, file))\n",
    "    \n",
    "    print(f\"Found {len(image_files)} image files\")\n",
    "    \n",
    "    if len(image_files) == 0:\n",
    "        print(\"No image files found. Downloading im2latex-100k dataset as a substitute...\")\n",
    "        # We'll use the im2latex-100k dataset as a substitute\n",
    "        !wget https://zenodo.org/record/56198/files/im2latex-100k.tgz\n",
    "        !tar xzf im2latex-100k.tgz\n",
    "        !mkdir -p {data_dir}/processed/images\n",
    "        !mv formula_images_processed/* {data_dir}/processed/images/\n",
    "        \n",
    "        # Process annotations\n",
    "        annotations = []\n",
    "        with open('im2latex_formulas.norm.lst', 'r') as f:\n",
    "            formulas = f.readlines()\n",
    "            \n",
    "        with open('im2latex_train.lst', 'r') as f:\n",
    "            for line in f:\n",
    "                parts = line.strip().split()\n",
    "                if len(parts) >= 2:\n",
    "                    image_id = parts[0]\n",
    "                    formula_id = int(parts[1])\n",
    "                    annotations.append({\n",
    "                        'image_path': os.path.join(data_dir, \"processed\", \"images\", f\"{image_id}.png\"),\n",
    "                        'formula': formulas[formula_id].strip()\n",
    "                    })\n",
    "    else:\n",
    "        # Try to match images with annotations\n",
    "        annotations = []\n",
    "        for img_path in image_files:\n",
    "            img_dir = os.path.dirname(img_path)\n",
    "            base_name = os.path.splitext(os.path.basename(img_path))[0]\n",
    "            \n",
    "            # Try to find annotation file with various extensions\n",
    "            annotation_content = None\n",
    "            for ext in ['.txt', '.tex', '.latex', '.mml']:\n",
    "                ann_path = os.path.join(img_dir, base_name + ext)\n",
    "                if os.path.exists(ann_path):\n",
    "                    with open(ann_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                        annotation_content = f.read().strip()\n",
    "                    break\n",
    "            \n",
    "            # If no annotation file found, try looking in a separate annotations directory\n",
    "            if annotation_content is None:\n",
    "                for ann_dir in ['annotations', 'formulas', 'labels']:\n",
    "                    potential_path = os.path.join(data_dir, ann_dir, base_name + '.txt')\n",
    "                    if os.path.exists(potential_path):\n",
    "                        with open(potential_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                            annotation_content = f.read().strip()\n",
    "                        break\n",
    "            \n",
    "            # Use empty string if still no annotation found\n",
    "            if annotation_content is None:\n",
    "                print(f\"No annotation found for {img_path}, using empty string\")\n",
    "                annotation_content = \"\"\n",
    "            \n",
    "            # Copy image to processed directory\n",
    "            dest_path = os.path.join(data_dir, \"processed\", \"images\", os.path.basename(img_path))\n",
    "            shutil.copy(img_path, dest_path)\n",
    "            \n",
    "            annotations.append({\n",
    "                'image_path': dest_path,\n",
    "                'formula': annotation_content\n",
    "            })\n",
    "    \n",
    "    # Create train/val/test splits\n",
    "    np.random.shuffle(annotations)\n",
    "    n = len(annotations)\n",
    "    train_idx = int(0.8 * n)\n",
    "    val_idx = int(0.9 * n)\n",
    "    \n",
    "    train_data = annotations[:train_idx]\n",
    "    val_data = annotations[train_idx:val_idx]\n",
    "    test_data = annotations[val_idx:]\n",
    "    \n",
    "    # Save splits\n",
    "    with open(os.path.join(data_dir, 'train.json'), 'w') as f:\n",
    "        json.dump(train_data, f)\n",
    "    with open(os.path.join(data_dir, 'val.json'), 'w') as f:\n",
    "        json.dump(val_data, f)\n",
    "    with open(os.path.join(data_dir, 'test.json'), 'w') as f:\n",
    "        json.dump(test_data, f)\n",
    "    \n",
    "    # Extract paths and formulas\n",
    "    train_images = [item['image_path'] for item in train_data]\n",
    "    train_formulas = [item['formula'] for item in train_data]\n",
    "    val_images = [item['image_path'] for item in val_data]\n",
    "    val_formulas = [item['formula'] for item in val_data]\n",
    "    test_images = [item['image_path'] for item in test_data]\n",
    "    test_formulas = [item['formula'] for item in test_data]\n",
    "    \n",
    "    print(f\"Data splits created: Train {len(train_images)} / Val {len(val_images)} / Test {len(test_images)}\")\n",
    "    \n",
    "    return {\n",
    "        'train': train_images, 'train_formulas': train_formulas,\n",
    "        'val': val_images, 'val_formulas': val_formulas,\n",
    "        'test': test_images, 'test_formulas': test_formulas\n",
    "    }\n",
    "\n",
    "# Try the fallback method if needed\n",
    "if 'train_images' not in locals() or len(train_images) == 0:\n",
    "    try:\n",
    "        print(\"Using fallback processing method...\")\n",
    "        splits = process_mathwriting_fallback(data_dir)\n",
    "        train_images, train_formulas = splits['train'], splits['train_formulas']\n",
    "        val_images, val_formulas = splits['val'], splits['val_formulas']\n",
    "        test_images, test_formulas = splits['test'], splits['test_formulas']\n",
    "    except Exception as e:\n",
    "        print(f\"Fallback processing also failed: {e}\")\n",
    "        print(\"Please check the dataset structure manually.\")\n",
    "\n",
    "# Print statistics\n",
    "print(\"\\nDataset Statistics:\")\n",
    "print(f\"Total samples: {len(train_images) + len(val_images) + len(test_images)}\")\n",
    "print(f\"Training samples: {len(train_images)}\")\n",
    "print(f\"Validation samples: {len(val_images)}\")\n",
    "print(f\"Test samples: {len(test_images)}\")\n",
    "\n",
    "# Show sample formulas\n",
    "print(\"\\nSample LaTeX formulas:\")\n",
    "for i, formula in enumerate(train_formulas[:3]):\n",
    "    print(f\"Sample {i+1}: {formula[:100]}{'...' if len(formula) > 100 else ''}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff15ce2e",
   "metadata": {},
   "source": [
    "## 3. Load TrOCR Model\n",
    "\n",
    "Load the pre-trained TrOCR_Math_handwritten model from Hugging Face."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9391d4f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the TrOCR processor and model\n",
    "model_name = \"fhswf/TrOCR_Math_handwritten\"\n",
    "\n",
    "# Load processor\n",
    "processor = TrOCRProcessor.from_pretrained(model_name)\n",
    "\n",
    "# Load model\n",
    "model = VisionEncoderDecoderModel.from_pretrained(model_name)\n",
    "\n",
    "# Move model to device (GPU if available)\n",
    "model = model.to(device)\n",
    "\n",
    "# Print model architecture summary\n",
    "print(f\"Model architecture: {model.__class__.__name__}\")\n",
    "print(f\"Encoder: {model.encoder.__class__.__name__}\")\n",
    "print(f\"Decoder: {model.decoder.__class__.__name__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "989c2336",
   "metadata": {},
   "source": [
    "## 4. Define Data Preprocessing\n",
    "\n",
    "Create functions to preprocess the handwritten math images and their corresponding LaTeX labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5d2535",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define max dimensions to ensure consistent sizes\n",
    "max_width = 384\n",
    "max_height = 384\n",
    "\n",
    "def preprocess_image(image_path):\n",
    "    \"\"\"Load and preprocess an image for the TrOCR model.\"\"\"\n",
    "    try:\n",
    "        # Open image\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        \n",
    "        # Resize while preserving aspect ratio\n",
    "        width, height = image.size\n",
    "        if width > max_width or height > max_height:\n",
    "            ratio = min(max_width / width, max_height / height)\n",
    "            new_width = int(width * ratio)\n",
    "            new_height = int(height * ratio)\n",
    "            image = image.resize((new_width, new_height), Image.LANCZOS)\n",
    "        \n",
    "        return image\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing image {image_path}: {e}\")\n",
    "        # Return a blank image as fallback\n",
    "        return Image.new(\"RGB\", (max_width, max_height), color=\"white\")\n",
    "\n",
    "def normalize_latex(formula):\n",
    "    \"\"\"Normalize LaTeX formula for consistent training.\"\"\"\n",
    "    # Remove unnecessary whitespace\n",
    "    formula = formula.strip()\n",
    "    # Replace multiple spaces with a single space\n",
    "    formula = \" \".join(formula.split())\n",
    "    return formula"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a24080a9",
   "metadata": {},
   "source": [
    "## 5. Create Dataset and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02319e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the processed data\n",
    "import json\n",
    "\n",
    "def load_dataset_split(split_file, base_dir):\n",
    "    \"\"\"Load a dataset split from a JSON file.\"\"\"\n",
    "    with open(split_file, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    images = []\n",
    "    formulas = []\n",
    "    \n",
    "    for item in data:\n",
    "        # Get image path\n",
    "        image_path = item['image_path']\n",
    "        # Make sure path is absolute if it isn't already\n",
    "        if not os.path.isabs(image_path):\n",
    "            image_path = os.path.join(base_dir, image_path)\n",
    "        \n",
    "        if os.path.exists(image_path):\n",
    "            images.append(image_path)\n",
    "            formulas.append(normalize_latex(item['formula']))\n",
    "        else:\n",
    "            print(f\"Warning: Image not found at {image_path}\")\n",
    "    \n",
    "    return images, formulas\n",
    "\n",
    "try:\n",
    "    # Try to load from JSON files created during dataset processing\n",
    "    train_images, train_formulas = load_dataset_split(os.path.join(data_dir, 'train.json'), data_dir)\n",
    "    val_images, val_formulas = load_dataset_split(os.path.join(data_dir, 'val.json'), data_dir)\n",
    "    test_images, test_formulas = load_dataset_split(os.path.join(data_dir, 'test.json'), data_dir)\n",
    "    \n",
    "    print(f\"Loaded {len(train_images)} training samples\")\n",
    "    print(f\"Loaded {len(val_images)} validation samples\")\n",
    "    print(f\"Loaded {len(test_images)} test samples\")\n",
    "    \n",
    "    # Display a sample image path\n",
    "    if train_images:\n",
    "        print(f\"Sample image path: {train_images[0]}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading dataset splits: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc8be372",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a PyTorch dataset\n",
    "class MathDataset(Dataset):\n",
    "    def __init__(self, image_paths, formulas, processor):\n",
    "        self.image_paths = image_paths\n",
    "        self.formulas = formulas\n",
    "        self.processor = processor\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Load and preprocess image\n",
    "        image = preprocess_image(self.image_paths[idx])\n",
    "        formula = self.formulas[idx]\n",
    "        \n",
    "        # Prepare for the model\n",
    "        pixel_values = self.processor(image, return_tensors=\"pt\").pixel_values.squeeze(0)\n",
    "        labels = self.processor.tokenizer(formula, \n",
    "                                          padding=\"max_length\",\n",
    "                                          max_length=512,\n",
    "                                          truncation=True,\n",
    "                                          return_tensors=\"pt\").input_ids.squeeze(0)\n",
    "        \n",
    "        return {\n",
    "            \"pixel_values\": pixel_values,\n",
    "            \"labels\": labels,\n",
    "        }\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = MathDataset(train_images, train_formulas, processor)\n",
    "val_dataset = MathDataset(val_images, val_formulas, processor)\n",
    "test_dataset = MathDataset(test_images, test_formulas, processor)\n",
    "\n",
    "# Verify an item from the dataset\n",
    "sample = train_dataset[0]\n",
    "print(f\"Pixel values shape: {sample['pixel_values'].shape}\")\n",
    "print(f\"Labels shape: {sample['labels'].shape}\")\n",
    "\n",
    "# Decode a sample to verify\n",
    "decoded_text = processor.tokenizer.decode(sample['labels'], skip_special_tokens=True)\n",
    "print(f\"Original formula: {train_formulas[0]}\")\n",
    "print(f\"Decoded formula: {decoded_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec1d2ea2",
   "metadata": {},
   "source": [
    "## 6. Set Up Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca60b03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up training configuration\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./trocr_math_finetuned\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=5,\n",
    "    predict_with_generate=True,\n",
    "    fp16=torch.cuda.is_available(),  # Use mixed precision training if available\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    report_to=\"none\",  # Disable wandb/tensorboard reporting\n",
    ")\n",
    "\n",
    "# Define metrics for evaluation\n",
    "cer_metric = evaluate.load(\"cer\")\n",
    "wer_metric = evaluate.load(\"wer\")\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels_ids = pred.label_ids\n",
    "    pred_ids = pred.predictions\n",
    "    \n",
    "    # Replace -100 with the pad_token_id\n",
    "    labels_ids[labels_ids == -100] = processor.tokenizer.pad_token_id\n",
    "    \n",
    "    # Decode predictions and references\n",
    "    pred_str = processor.tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    labels_ids[labels_ids == -100] = processor.tokenizer.pad_token_id\n",
    "    label_str = processor.tokenizer.batch_decode(labels_ids, skip_special_tokens=True)\n",
    "    \n",
    "    # Compute metrics\n",
    "    cer = cer_metric.compute(predictions=pred_str, references=label_str)\n",
    "    wer = wer_metric.compute(predictions=pred_str, references=label_str)\n",
    "    \n",
    "    return {\n",
    "        \"cer\": cer,\n",
    "        \"wer\": wer,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a5098b",
   "metadata": {},
   "source": [
    "## 7. Fine-tune the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941e0a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize trainer\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=processor.tokenizer,\n",
    "    data_collator=default_data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "print(\"Starting training...\")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f0213d",
   "metadata": {},
   "source": [
    "## 8. Evaluate Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "409ff2e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on the test set\n",
    "print(\"Evaluating on test set...\")\n",
    "results = trainer.evaluate(test_dataset)\n",
    "\n",
    "print(\"Test results:\")\n",
    "for key, value in results.items():\n",
    "    print(f\"{key}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "328d1fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to visualize predictions\n",
    "def visualize_predictions(model, processor, dataset, num_samples=5):\n",
    "    \"\"\"Visualize model predictions on a sample of images.\"\"\"\n",
    "    model.eval()\n",
    "    indices = np.random.choice(len(dataset), num_samples, replace=False)\n",
    "    \n",
    "    plt.figure(figsize=(15, 4 * num_samples))\n",
    "    \n",
    "    for i, idx in enumerate(indices):\n",
    "        # Get sample\n",
    "        sample = dataset[idx]\n",
    "        pixel_values = sample[\"pixel_values\"].unsqueeze(0).to(device)\n",
    "        \n",
    "        # Generate prediction\n",
    "        with torch.no_grad():\n",
    "            generated_ids = model.generate(\n",
    "                pixel_values,\n",
    "                max_length=512,\n",
    "                num_beams=4,\n",
    "                early_stopping=True\n",
    "            )\n",
    "        \n",
    "        # Decode prediction\n",
    "        pred_str = processor.tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "        \n",
    "        # Get ground truth\n",
    "        gt_str = processor.tokenizer.decode(sample[\"labels\"], skip_special_tokens=True)\n",
    "        \n",
    "        # Reload image for display\n",
    "        image_path = dataset.image_paths[idx]\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        \n",
    "        # Display\n",
    "        plt.subplot(num_samples, 1, i+1)\n",
    "        plt.imshow(image)\n",
    "        plt.title(f\"Ground Truth: {gt_str}\\nPrediction: {pred_str}\", fontsize=12)\n",
    "        plt.axis(\"off\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize some predictions\n",
    "visualize_predictions(model, processor, test_dataset, num_samples=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ab970a",
   "metadata": {},
   "source": [
    "## 9. Save the Fine-tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a72a1734",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the fine-tuned model\n",
    "output_dir = \"./trocr_math_finetuned_final\"\n",
    "model.save_pretrained(output_dir)\n",
    "processor.save_pretrained(output_dir)\n",
    "print(f\"Model and processor saved to {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98bbc6b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Push the model to HuggingFace Hub (optional)\n",
    "# You need to be logged in to the HuggingFace Hub\n",
    "# !pip install -q huggingface_hub\n",
    "# from huggingface_hub import notebook_login\n",
    "# notebook_login()\n",
    "\n",
    "# # Define your HuggingFace Hub repository\n",
    "# hf_repo_name = \"your-username/trocr-math-finetuned\"\n",
    "\n",
    "# # Upload model and processor\n",
    "# model.push_to_hub(hf_repo_name)\n",
    "# processor.push_to_hub(hf_repo_name)\n",
    "# print(f\"Model pushed to {hf_repo_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5696571e",
   "metadata": {},
   "source": [
    "## 10. Test the Fine-tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1855047b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the fine-tuned model\n",
    "finetuned_model_path = \"./trocr_math_finetuned_final\"\n",
    "finetuned_processor = TrOCRProcessor.from_pretrained(finetuned_model_path)\n",
    "finetuned_model = VisionEncoderDecoderModel.from_pretrained(finetuned_model_path).to(device)\n",
    "\n",
    "# Test on a custom image (you can provide a path to any handwritten math expression image)\n",
    "def predict_from_image(image_path, processor, model):\n",
    "    # Load and preprocess image\n",
    "    image = preprocess_image(image_path)\n",
    "    pixel_values = processor(image, return_tensors=\"pt\").pixel_values.to(device)\n",
    "    \n",
    "    # Generate prediction\n",
    "    with torch.no_grad():\n",
    "        generated_ids = model.generate(\n",
    "            pixel_values,\n",
    "            max_length=512,\n",
    "            num_beams=4,\n",
    "            early_stopping=True\n",
    "        )\n",
    "    \n",
    "    # Decode prediction\n",
    "    pred_str = processor.tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Display\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.imshow(image)\n",
    "    plt.title(f\"Predicted: {pred_str}\", fontsize=12)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "    \n",
    "    return pred_str\n",
    "\n",
    "# Test on a sample from the test set\n",
    "if len(test_images) > 0:\n",
    "    sample_image_path = test_images[0]\n",
    "    prediction = predict_from_image(sample_image_path, finetuned_processor, finetuned_model)\n",
    "    print(f\"Predicted LaTeX: {prediction}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7606b66",
   "metadata": {},
   "source": [
    "## 11. Comparison with Original Model\n",
    "\n",
    "Compare the fine-tuned model with the original pre-trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7168d009",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the original model again\n",
    "original_processor = TrOCRProcessor.from_pretrained(\"fhswf/TrOCR_Math_handwritten\")\n",
    "original_model = VisionEncoderDecoderModel.from_pretrained(\"fhswf/TrOCR_Math_handwritten\").to(device)\n",
    "\n",
    "# Function to compare models\n",
    "def compare_models(image_path, original_model, finetuned_model, processor):\n",
    "    # Load and preprocess image\n",
    "    image = preprocess_image(image_path)\n",
    "    pixel_values = processor(image, return_tensors=\"pt\").pixel_values.to(device)\n",
    "    \n",
    "    # Generate predictions\n",
    "    with torch.no_grad():\n",
    "        original_ids = original_model.generate(\n",
    "            pixel_values,\n",
    "            max_length=512,\n",
    "            num_beams=4,\n",
    "            early_stopping=True\n",
    "        )\n",
    "        \n",
    "        finetuned_ids = finetuned_model.generate(\n",
    "            pixel_values,\n",
    "            max_length=512,\n",
    "            num_beams=4,\n",
    "            early_stopping=True\n",
    "        )\n",
    "    \n",
    "    # Decode predictions\n",
    "    original_str = processor.tokenizer.decode(original_ids[0], skip_special_tokens=True)\n",
    "    finetuned_str = processor.tokenizer.decode(finetuned_ids[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Display\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.imshow(image)\n",
    "    plt.title(f\"Original: {original_str}\\nFine-tuned: {finetuned_str}\", fontsize=12)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "    \n",
    "    return original_str, finetuned_str\n",
    "\n",
    "# Test on several samples\n",
    "if len(test_images) >= 3:\n",
    "    for i in range(3):\n",
    "        sample_image_path = test_images[i]\n",
    "        original, finetuned = compare_models(sample_image_path, original_model, finetuned_model, processor)\n",
    "        print(f\"Sample {i+1}:\")\n",
    "        print(f\"Original model: {original}\")\n",
    "        print(f\"Fine-tuned model: {finetuned}\")\n",
    "        print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "679cddc9",
   "metadata": {},
   "source": [
    "## 12. Integration with Your Existing Code\n",
    "\n",
    "Here's how to integrate the fine-tuned model with your existing `multimodal_ocr.py` code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4815554c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example integration code\n",
    "'''\n",
    "# Add this to your MODEL_OPTIONS in multimodal_ocr.py\n",
    "MODEL_OPTIONS = {\n",
    "    \"Nanonets\": \"nanonets/Nanonets-OCR-s\",\n",
    "    \"PrithivMLmods\" : \"prithivMLmods/Qwen2-VL-OCR-2B-Instruct\",\n",
    "    \"Custom_TrOCR_Math\": \"./trocr_math_finetuned_final\"  # Path to your fine-tuned model\n",
    "}\n",
    "\n",
    "# Add this to your model loading code\n",
    "elif name == \"Custom_TrOCR_Math\":\n",
    "    from transformers import TrOCRProcessor, VisionEncoderDecoderModel\n",
    "    models[name] = VisionEncoderDecoderModel.from_pretrained(\n",
    "        model_id,\n",
    "        torch_dtype=torch.float16\n",
    "    ).to(\"cuda\").eval()\n",
    "    processors[name] = TrOCRProcessor.from_pretrained(model_id)\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
